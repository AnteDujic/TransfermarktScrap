{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa847b46-1661-4820-95e5-dbbb75c8b4fe",
   "metadata": {},
   "source": [
    "# DATA SCRAPING\n",
    "## AUTHOR: ANTE DUJIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd6a762-62a7-4111-b236-1c931aab2b7d",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3354a878-e5ab-4747-805e-d5209083ad77",
   "metadata": {},
   "source": [
    "Data scraping, also known as web scraping, is the process of automatically extracting data from websites using various software tools. It is an important skill for data analysis as it allows gathering of large amounts of data from the internet quickly and efficiently.To successfully write a web scraping script, it is important to have familiarity with programming, but also to have a fundamental understanding of HTML structure and the basics of web development. It's also crucial to approach data scraping with caution and ensure responsible use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ca1fd1-70aa-4c1f-88f4-f7f227acf04d",
   "metadata": {},
   "source": [
    "<img src=\"scrap.jpg\" width=\"400\" style=\"margin:auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f4f21-288d-4c92-b75f-da332869c65b",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook, I will explore web scraping by creating a script to extract data from [Transfermarkt](https://www.transfermarkt.com/), a popular website specializing in football player and team statistics, transfers, and market values. I will be using Python and it's libraries - Beautiful Soup for parsing HTML and requests for making HTTP requests.\n",
    "\n",
    "To create a web scraper I will follow the steps below:\n",
    "\n",
    "1. Identifying the target data\n",
    "2. Inspect the structure of the website\n",
    "3. Write Code to Navigate Through the HTML Structure\n",
    "4. Creating a DataFrame\n",
    "5. Exporting the Data to CSV\n",
    "\n",
    "It is important to note that this project is designed for educational purposes only. The data obtained through scraping will not be used for any other means beyond learning and exploration. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38409ba-259b-4634-aeed-8c4eee6687a6",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db684e59-747b-4524-935f-618c76f187b4",
   "metadata": {},
   "source": [
    "### STEP 1. Identifying the target data\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95955c0-124c-4a61-86d9-4d960978bd5e",
   "metadata": {},
   "source": [
    "My objective is to extract data on all players in the Croatian football league ([Supersport HNL](https://www.transfermarkt.com/1-hnl/startseite/wettbewerb/KR1)), including their name, age, nationality, position, height, and more. To do so, I will have to navigate to each club url. I will then create a CSV file with this data, which can be used for further analysis.\n",
    "\n",
    "Example of a target data for single club: https://www.transfermarkt.com/gnk-dinamo-zagreb/kader/verein/419/saison_id/2022/plus/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6cc773-e26d-4a54-a1dd-7d25fa644ddc",
   "metadata": {},
   "source": [
    "### STEP 2. Inspect the structure of the website\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1850d3a5-29b6-40f1-888a-15463f674631",
   "metadata": {},
   "source": [
    "Before starting to write the code, I will inspect the website itself using browser developer tools to directly observe the actual webpage and validate my understanding of its structure. I will use my understanding of the parsed HTML structure to extract the relevant data. This involves accessing specific elements, retrieving their attributes or text content, and navigating through the HTML structure to extract the precise information I need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c53aca1-102d-4812-a245-d0392240199d",
   "metadata": {},
   "source": [
    "### STEP 3. Write Code to Navigate Through the HTML Structure\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76f2b7e-ac10-4968-96b2-d2460f9d219f",
   "metadata": {},
   "source": [
    "Here I'll write the code for the scraper. I will do this in two stages. First I will construct the links where I want to collect the data for further analysis. I will then scrap the data from those links and create a single database. This section is divided into three parts: Importing Libraries, Constructing Club URLs and Scraping Player Data. More on each below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6036b5d3-2881-4288-bcba-764e8d4fa8fd",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9021ea4d-f4e1-4bd1-8a17-ec0903c8352f",
   "metadata": {},
   "source": [
    "First, I import the necessary libraries for web scraping and data processing tasks. The *requests* library allows me to make HTTP requests to retrieve web page content. I use *BeautifulSoup* to parse the HTML content and extract specific elements of interest. Finally, I import *pandas* to create and manipulate data frames for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd28536-ad75-43b7-92bd-6c6ca9785380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad0e42f-80f2-4544-8d47-11e86c13bad1",
   "metadata": {},
   "source": [
    "#### Constructing Club URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d1bda-77c7-416e-973d-5bf6ba086d57",
   "metadata": {},
   "source": [
    "In this section, I will construct the URLs of the each club in the league. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1c1f44-e327-46f4-a3ab-c5f16e612b94",
   "metadata": {},
   "source": [
    "I will begin with an initial link that serves as the starting point to access the website's content. This link leads us to a page related to a specific league or competition - Supersport HNL in our case. After this I will extract the information that includes links to individual clubs participating in the league. Using the extracted information, I construct a new URL for each club by appending the gathered information to a base URL. This new URL directs us to a dedicated page containing information about the club. Once I have the link for each club, I will manipulate the URL further to construct the final link. This involves replacing specific parts of the URL to navigate to the desired page that holds the target data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20769f6-cd35-48e2-ab34-657645566a51",
   "metadata": {},
   "source": [
    "##### EXAMPLE: GNK Dinamo Zagreb\n",
    "$\\rightarrow$ INITIAL LINK: https://www.transfermarkt.com/1-hnl/startseite/wettbewerb/KR1 </br> \n",
    "&nbsp; &nbsp;$\\rightarrow$ SECOND LINK: https://www.transfermarkt.com/gnk-dinamo-zagreb/startseite/verein/419/saison_id/2022 </br> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\rightarrow$ FINAL LINK: https://www.transfermarkt.com/gnk-dinamo-zagreb/kader/verein/419/saison_id/2022/plus/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6bb2d-73d8-4480-8ed7-a62de1ef9198",
   "metadata": {},
   "source": [
    "To do so, I will use the requests library to make a GET request to the website and fetch its content. Then, I will use BeautifulSoup to parse the HTML content and find the relevant elements containing the club URLs. I will store these URLs in a list for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf1de86-7475-4dc6-aa25-928a6829ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supersport HNL url\n",
    "    # Initial link\n",
    "url = 'https://www.transfermarkt.com/1-hnl/startseite/wettbewerb/KR1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b4a34c-ab67-4c60-8928-883df951b297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Defining a dictionary named headers, to mimic the user agent of the browser. \n",
    "# This is necessary because some websites may block requests from bots or scripts that are not coming from a browser.\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "# Send a GET request to the specified URL with the defined headers\n",
    "response = requests.get(url, headers=headers)\n",
    "# <Response [200]> indicates that the GET request to the specified URL was successful.\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc07f21a-626d-4444-b008-fb8fbb734eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the content of the response\n",
    "content = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eb6c4a0-84eb-469b-92fa-356dff61b9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47f9404e-1eaf-40d4-86fc-b8953e7f453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the div element with class 'responsive-table' which contains the table\n",
    "table = soup.find('div', {'class': 'responsive-table'})\n",
    "#table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88c30194-248d-4b64-92e4-036a67440117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the tbody element within the table\n",
    "tbody = table.find('tbody')\n",
    "#tbody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15b805ae-9e3b-4c83-bba9-62ade869896d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.transfermarkt.com/gnk-dinamo-zagreb/kader/verein/419/saison_id/2022/plus/1',\n",
       " 'https://www.transfermarkt.com/hnk-hajduk-split/kader/verein/447/saison_id/2022/plus/1',\n",
       " 'https://www.transfermarkt.com/hnk-rijeka/kader/verein/144/saison_id/2022/plus/1',\n",
       " 'https://www.transfermarkt.com/nk-lokomotiva-zagreb/kader/verein/11194/saison_id/2022/plus/1',\n",
       " 'https://www.transfermarkt.com/nk-osijek/kader/verein/327/saison_id/2022/plus/1',\n",
       " 'https://www.transfermarkt.com/hnk-gorica/kader/verein/24575/saison_id/2022/plus/1',\n",
       " 'https://www.transfermarkt.com/nk-varazdin/kader/verein/599/saison_id/2022/plus/1',\n",
       " 'https://www.transfermarkt.com/nk-istra-1961/kader/verein/999/saison_id/2022/plus/1',\n",
       " 'https://www.transfermarkt.com/hnk-sibenik/kader/verein/223/saison_id/2022/plus/1',\n",
       " 'https://www.transfermarkt.com/slaven-belupo-koprivnica/kader/verein/2362/saison_id/2022/plus/1']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty list to store the club links\n",
    "club_links = []\n",
    "\n",
    "# Iterate over each tr element within the tbody\n",
    "for tr in tbody.find_all('tr'):\n",
    "    # Get the href attribute of the first 'a' element and append it to the club_links list\n",
    "    club_links.append(tr.find_all('a')[0]['href'])\n",
    "\n",
    "# Prepend the base URL and modify the links to include '/kader/plus/1'\n",
    "for i in range(len(club_links)):\n",
    "    # Second link\n",
    "    club_links[i] = 'https://www.transfermarkt.com' + club_links[i]\n",
    "    # Final link\n",
    "    club_links[i] = club_links[i].replace('startseite', 'kader') + '/plus/1'\n",
    "\n",
    "# Sanity check: Print the list of club links\n",
    "club_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35c6493-0aed-4eba-ab87-b60f2eddf9e6",
   "metadata": {},
   "source": [
    "#### Scraping Player Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990177e1-993e-4690-9c0a-9dd6f514c359",
   "metadata": {},
   "source": [
    "In this section, I will scrape the player data for each club. I'll iterate through the club URLs obtained in the previous section and make individual requests to each club's page. Using BeautifulSoup, I'll extract the desired player information such as name, position, nationality, height, etc. This data is then stored in a dictionary and appended to a list, accumulating data for all players from different clubs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f0ad902-b5b3-4cf6-92cb-a3cafb486e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty list to store all players' data\n",
    "all_players = []\n",
    "\n",
    "# Iterate over each club URL\n",
    "for clubs in club_links:\n",
    "    # Assign the club URL to club_url variable\n",
    "    club_url = clubs\n",
    "    \n",
    "    # Define the headers dictionary to mimic the user agent of the browser\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    \n",
    "    # Send a GET request to the club URL with the defined headers\n",
    "    response = requests.get(club_url, headers=headers)\n",
    "    \n",
    "    # Get the content of the response\n",
    "    content = response.content\n",
    "    \n",
    "    # Parse the content using BeautifulSoup\n",
    "    club_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the table element with class 'items' which contains the player data\n",
    "    player_table = club_soup.find('table', {'class': 'items'})\n",
    "    \n",
    "    # Select all 'tr' elements with class 'odd' or 'even' within the player_table\n",
    "    players = player_table.select('tr.odd, tr.even')\n",
    "    \n",
    "    # Iterate over the players\n",
    "    for i in range(1, len(players) + 1):\n",
    "        # Create an empty list to store the nationalities of the player\n",
    "        nationalities = []\n",
    "        \n",
    "        # Find all 'img' elements with class 'flaggenrahmen' within the current player row\n",
    "        imgs = players[i - 1].find_all('img', {'class': 'flaggenrahmen'})\n",
    "        \n",
    "        # Iterate over the 'img' elements to extract the nationalities\n",
    "        for img in imgs:\n",
    "            nationalities.append(img.get('alt'))\n",
    "        \n",
    "        # Find the 'img' element with an empty class, representing the club the player joined from\n",
    "        joined_from_element = players[i - 1].find('img', {'class': ''})\n",
    "        \n",
    "        # Check if the joined_from_element exists and extract its 'alt' attribute,\n",
    "        # otherwise, assign an empty string to joined_from\n",
    "        if joined_from_element:\n",
    "            joined_from = joined_from_element.get('alt')\n",
    "        else:\n",
    "            joined_from = \"\"\n",
    "        \n",
    "        # Create a dictionary to store the player data\n",
    "        player_dict = {\n",
    "            'number': players[i - 1].find('div', {'class': 'rn_nummer'}).text.strip(),\n",
    "            'image_url': players[i - 1].find('img', {'class': 'bilderrahmen-fixed'}).get('data-src'),\n",
    "            'name': players[i - 1].find('td', {'class': 'hauptlink'}).text.strip(),\n",
    "            'position': players[i - 1].find_all('td')[4].text.strip(),\n",
    "            'dob': players[i - 1].find_all('td', {'class': 'zentriert'})[1].text.strip()[:-5],\n",
    "            'nationality': ', '.join(nationalities),\n",
    "            'height': players[i - 1].find_all('td', {'class': 'zentriert'})[3].text.strip()[:-1],\n",
    "            'foot': players[i - 1].find_all('td', {'class': 'zentriert'})[4].text.strip(),\n",
    "            'joined': players[i - 1].find_all('td', {'class': 'zentriert'})[5].text.strip(),\n",
    "            'joined_from': joined_from,\n",
    "            'contract': players[i - 1].find_all('td', {'class': 'zentriert'})[7].text.strip(),\n",
    "            'value': players[i - 1].find('td', {'class': 'rechts hauptlink'}).text.strip()[1:],\n",
    "            'club_name': club_soup.find('h1', {'class': 'data-header__headline-wrapper data-header__headline-wrapper--oswald'}).text.strip()\n",
    "        }\n",
    "        \n",
    "        # Append the player data dictionary to the all_players list\n",
    "        all_players.append(player_dict)\n",
    "\n",
    "# Sanity check: Print the number of players scraped\n",
    "len(all_players)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf62be8-7873-4f8c-abf0-9f9a52f7d0d5",
   "metadata": {},
   "source": [
    "### STEP 4. Creating Dataframe\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04061ea4-477d-43a3-b4ef-f9ca71f18698",
   "metadata": {},
   "source": [
    "In this section, I create a pandas data frame to organize and structure the scraped player data. The data frame allows me to perform various operations and analysis on the data more conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70aa17f5-9e34-4c31-9862-58fe3ef298fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>image_url</th>\n",
       "      <th>name</th>\n",
       "      <th>position</th>\n",
       "      <th>dob</th>\n",
       "      <th>nationality</th>\n",
       "      <th>height</th>\n",
       "      <th>foot</th>\n",
       "      <th>joined</th>\n",
       "      <th>joined_from</th>\n",
       "      <th>contract</th>\n",
       "      <th>value</th>\n",
       "      <th>club_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>https://img.a.transfermarkt.technology/portrai...</td>\n",
       "      <td>Dominik Livakovic</td>\n",
       "      <td>Goalkeeper</td>\n",
       "      <td>Jan 9, 1995</td>\n",
       "      <td>Croatia</td>\n",
       "      <td>1,88</td>\n",
       "      <td>right</td>\n",
       "      <td>Aug 31, 2015</td>\n",
       "      <td>NK Zagreb</td>\n",
       "      <td>Jun 15, 2024</td>\n",
       "      <td>14.00m</td>\n",
       "      <td>GNK Dinamo Zagreb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>https://img.a.transfermarkt.technology/portrai...</td>\n",
       "      <td>Ivan Nevistic</td>\n",
       "      <td>Goalkeeper</td>\n",
       "      <td>Jul 31, 1998</td>\n",
       "      <td>Croatia</td>\n",
       "      <td>1,95</td>\n",
       "      <td>right</td>\n",
       "      <td>Jan 28, 2021</td>\n",
       "      <td>NK Lokomotiva Zagreb</td>\n",
       "      <td>Jun 15, 2025</td>\n",
       "      <td>1.50m</td>\n",
       "      <td>GNK Dinamo Zagreb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>https://img.a.transfermarkt.technology/portrai...</td>\n",
       "      <td>Danijel Zagorac</td>\n",
       "      <td>Goalkeeper</td>\n",
       "      <td>Feb 7, 1987</td>\n",
       "      <td>Croatia</td>\n",
       "      <td>1,86</td>\n",
       "      <td>right</td>\n",
       "      <td>Jul 11, 2016</td>\n",
       "      <td>RNK Split</td>\n",
       "      <td>Jun 30, 2026</td>\n",
       "      <td>200k</td>\n",
       "      <td>GNK Dinamo Zagreb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>https://img.a.transfermarkt.technology/portrai...</td>\n",
       "      <td>Josip Sutalo</td>\n",
       "      <td>Centre-Back</td>\n",
       "      <td>Feb 28, 2000</td>\n",
       "      <td>Croatia</td>\n",
       "      <td>1,90</td>\n",
       "      <td>right</td>\n",
       "      <td>Jan 7, 2020</td>\n",
       "      <td>GNK Dinamo Zagreb II</td>\n",
       "      <td>Jun 15, 2028</td>\n",
       "      <td>18.00m</td>\n",
       "      <td>GNK Dinamo Zagreb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>https://img.a.transfermarkt.technology/portrai...</td>\n",
       "      <td>Dino Peric</td>\n",
       "      <td>Centre-Back</td>\n",
       "      <td>Jul 12, 1994</td>\n",
       "      <td>Croatia</td>\n",
       "      <td>1,97</td>\n",
       "      <td>left</td>\n",
       "      <td>Jul 7, 2017</td>\n",
       "      <td>NK Lokomotiva Zagreb</td>\n",
       "      <td>Jun 15, 2026</td>\n",
       "      <td>5.00m</td>\n",
       "      <td>GNK Dinamo Zagreb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  number                                          image_url  \\\n",
       "0     40  https://img.a.transfermarkt.technology/portrai...   \n",
       "1     33  https://img.a.transfermarkt.technology/portrai...   \n",
       "2      1  https://img.a.transfermarkt.technology/portrai...   \n",
       "3     37  https://img.a.transfermarkt.technology/portrai...   \n",
       "4     55  https://img.a.transfermarkt.technology/portrai...   \n",
       "\n",
       "                name     position           dob nationality height   foot  \\\n",
       "0  Dominik Livakovic   Goalkeeper   Jan 9, 1995     Croatia   1,88  right   \n",
       "1      Ivan Nevistic   Goalkeeper  Jul 31, 1998     Croatia   1,95  right   \n",
       "2    Danijel Zagorac   Goalkeeper   Feb 7, 1987     Croatia   1,86  right   \n",
       "3       Josip Sutalo  Centre-Back  Feb 28, 2000     Croatia   1,90  right   \n",
       "4         Dino Peric  Centre-Back  Jul 12, 1994     Croatia   1,97   left   \n",
       "\n",
       "         joined           joined_from      contract   value          club_name  \n",
       "0  Aug 31, 2015             NK Zagreb  Jun 15, 2024  14.00m  GNK Dinamo Zagreb  \n",
       "1  Jan 28, 2021  NK Lokomotiva Zagreb  Jun 15, 2025   1.50m  GNK Dinamo Zagreb  \n",
       "2  Jul 11, 2016             RNK Split  Jun 30, 2026    200k  GNK Dinamo Zagreb  \n",
       "3   Jan 7, 2020  GNK Dinamo Zagreb II  Jun 15, 2028  18.00m  GNK Dinamo Zagreb  \n",
       "4   Jul 7, 2017  NK Lokomotiva Zagreb  Jun 15, 2026   5.00m  GNK Dinamo Zagreb  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame from the list of player dictionaries\n",
    "df = pd.DataFrame(all_players)\n",
    "\n",
    "# Sanity check: Print the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bec0dd-3da3-441d-902e-4ccfac7aa63b",
   "metadata": {},
   "source": [
    "### STEP 5. Exporting the Data to CSV\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddd4c4b-262e-457b-a598-381ee1144666",
   "metadata": {},
   "source": [
    "In this section, I export the scraped player data to a CSV (Comma-Separated Values) file. By using the to_csv() function provided by pandas data frames, I convert the data frame into a CSV format. This allows me to save the data to a file that can be easily shared, stored, or further processed in other applications. The CSV file preserves the tabular structure of the data, making it accessible for analysis or visualization tasks outside of the current Jupyter Notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "885085de-94bb-4820-9a21-6dddb0707c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the DataFrame to a CSV file\n",
    "#df.to_csv('SuperSportHNL.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07649621-c464-4000-bb6c-440f0b66667d",
   "metadata": {},
   "source": [
    "***\n",
    "## THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
